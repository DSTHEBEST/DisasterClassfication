{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T10:49:53.488048Z",
     "start_time": "2025-04-03T10:48:38.410649Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(\"earthquke.csv\")  \n",
    "\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df[\"timestamp\"] = df[\"time\"].apply(lambda x: x.timestamp())\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_cols = [\"magType\", \"locationSource\", \"magSource\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "features = [\"timestamp\", \"latitude\", \"longitude\", \"depth\", \"magType\",\n",
    "            \"gap\", \"dmin\", \"rms\", \"horizontalError\", \"depthError\", \"magError\", \"locationSource\", \"magSource\"]\n",
    "\n",
    "target = \"mag\"  \n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "df[features] = feature_scaler.fit_transform(df[features])\n",
    "\n",
    "target_scaler = MinMaxScaler()\n",
    "df[target] = target_scaler.fit_transform(df[[target]])\n",
    "\n",
    "def create_sequences(data, target_data, seq_length=20):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(target_data[i+seq_length])  \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 20  \n",
    "data_values = df[features].values\n",
    "target_values = df[target].values\n",
    "\n",
    "X, y = create_sequences(data_values, target_values, seq_length)\n",
    "\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(256, return_sequences=True), input_shape=(seq_length, len(features))),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=32)\n",
    "\n",
    "model.save(\"/mnt/data/earthquake_lstm_model.h5\")\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predicted_magnitude = target_scaler.inverse_transform(predictions)\n",
    "\n",
    "print(predicted_magnitude[:10])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEVANSH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 63ms/step - loss: 0.0860 - mae: 0.2371 - val_loss: 0.0421 - val_mae: 0.1834\n",
      "Epoch 2/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 48ms/step - loss: 0.0357 - mae: 0.1639 - val_loss: 0.0231 - val_mae: 0.1330\n",
      "Epoch 3/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 50ms/step - loss: 0.0294 - mae: 0.1505 - val_loss: 0.0228 - val_mae: 0.1311\n",
      "Epoch 4/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 50ms/step - loss: 0.0297 - mae: 0.1513 - val_loss: 0.0232 - val_mae: 0.1333\n",
      "Epoch 5/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 51ms/step - loss: 0.0319 - mae: 0.1540 - val_loss: 0.0230 - val_mae: 0.1322\n",
      "Epoch 6/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 51ms/step - loss: 0.0306 - mae: 0.1545 - val_loss: 0.0228 - val_mae: 0.1309\n",
      "Epoch 7/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 80ms/step - loss: 0.0310 - mae: 0.1549 - val_loss: 0.0230 - val_mae: 0.1321\n",
      "Epoch 8/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 93ms/step - loss: 0.0310 - mae: 0.1536 - val_loss: 0.0229 - val_mae: 0.1316\n",
      "Epoch 9/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 73ms/step - loss: 0.0314 - mae: 0.1558 - val_loss: 0.0236 - val_mae: 0.1358\n",
      "Epoch 10/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 67ms/step - loss: 0.0306 - mae: 0.1539 - val_loss: 0.0227 - val_mae: 0.1302\n",
      "Epoch 11/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 61ms/step - loss: 0.0301 - mae: 0.1507 - val_loss: 0.0231 - val_mae: 0.1326\n",
      "Epoch 12/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 62ms/step - loss: 0.0295 - mae: 0.1515 - val_loss: 0.0226 - val_mae: 0.1295\n",
      "Epoch 13/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 83ms/step - loss: 0.0296 - mae: 0.1511 - val_loss: 0.0229 - val_mae: 0.1314\n",
      "Epoch 14/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 75ms/step - loss: 0.0302 - mae: 0.1519 - val_loss: 0.0229 - val_mae: 0.1311\n",
      "Epoch 15/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 81ms/step - loss: 0.0308 - mae: 0.1540 - val_loss: 0.0231 - val_mae: 0.1328\n",
      "Epoch 16/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 92ms/step - loss: 0.0320 - mae: 0.1546 - val_loss: 0.0232 - val_mae: 0.1334\n",
      "Epoch 17/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 72ms/step - loss: 0.0310 - mae: 0.1548 - val_loss: 0.0231 - val_mae: 0.1325\n",
      "Epoch 18/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 64ms/step - loss: 0.0302 - mae: 0.1525 - val_loss: 0.0231 - val_mae: 0.1329\n",
      "Epoch 19/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 67ms/step - loss: 0.0310 - mae: 0.1530 - val_loss: 0.0230 - val_mae: 0.1318\n",
      "Epoch 20/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 81ms/step - loss: 0.0296 - mae: 0.1502 - val_loss: 0.0234 - val_mae: 0.1344\n",
      "Epoch 21/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 76ms/step - loss: 0.0322 - mae: 0.1565 - val_loss: 0.0234 - val_mae: 0.1344\n",
      "Epoch 22/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 71ms/step - loss: 0.0306 - mae: 0.1542 - val_loss: 0.0230 - val_mae: 0.1323\n",
      "Epoch 23/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 89ms/step - loss: 0.0308 - mae: 0.1539 - val_loss: 0.0228 - val_mae: 0.1306\n",
      "Epoch 24/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 78ms/step - loss: 0.0313 - mae: 0.1548 - val_loss: 0.0225 - val_mae: 0.1286\n",
      "Epoch 25/25\n",
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 72ms/step - loss: 0.0299 - mae: 0.1517 - val_loss: 0.0235 - val_mae: 0.1350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 86ms/step\n",
      "[[3.8199797]\n",
      " [3.8199797]\n",
      " [3.8199797]\n",
      " [3.8199797]\n",
      " [3.8199797]\n",
      " [3.8199797]\n",
      " [3.8199797]\n",
      " [3.8199797]\n",
      " [3.8199797]\n",
      " [3.8199797]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T10:35:17.272818Z",
     "start_time": "2025-04-03T10:35:16.598136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"earthquke.csv\")  \n",
    "\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df[\"timestamp\"] = df[\"time\"].apply(lambda x: x.timestamp())\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_cols = [\"magType\", \"locationSource\", \"magSource\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "features = [\"timestamp\", \"latitude\", \"longitude\", \"depth\", \"mag\", \"magType\",\n",
    "            \"gap\", \"dmin\", \"rms\", \"horizontalError\", \"depthError\", \"magError\", \"locationSource\", \"magSource\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "seq_length = 10\n",
    "\n",
    "def create_sequences(data, seq_length=10):\n",
    "    X = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "    return np.array(X)\n",
    "\n",
    "data_values = df[features].values\n",
    "X_input = create_sequences(data_values, seq_length)\n",
    "\n",
    "model = tf.keras.models.load_model(\"earthquake_lstm_model.h5\", compile=False)\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['mae'])\n",
    "\n",
    "predictions = model.predict(X_input)\n",
    "\n",
    "predicted_magnitude = scaler.inverse_transform(\n",
    "    np.concatenate((data_values[seq_length:, :4], predictions, data_values[seq_length:, 5:]), axis=1)\n",
    ")[:, 4]\n",
    "\n",
    "print(predicted_magnitude)\n"
   ],
   "id": "8f5f2ea8d3ecef4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step\n",
      "[3.85117736 3.85117736 3.85117736 ... 3.85117736 3.85117736 3.85117736]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 1. Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'path_to_train_dataset',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'path_to_test_dataset',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ✅ 2. Load Pretrained EfficientNetB3 (instead of MobileNetV2)\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False  # Freeze base model\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.3)(x)  # Regularization\n",
    "output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# ✅ 3. Compute Class Weights (to handle imbalance)\n",
    "y_true = train_generator.classes\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_true), y=y_true)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# ✅ 4. Compile Model with Adam Optimizer & Learning Rate Scheduler\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "# ✅ 5. Train the Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=test_generator,\n",
    "    epochs=20,\n",
    "    class_weight=class_weight_dict,  # Apply class weights\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "# ✅ 6. Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
   ],
   "id": "ff96fcac7edbabc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
